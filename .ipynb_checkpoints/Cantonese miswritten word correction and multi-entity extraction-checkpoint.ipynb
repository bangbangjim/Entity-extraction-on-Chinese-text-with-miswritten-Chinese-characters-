{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here multi-entity extractions was attempted using a phonetic matching approach on Cantonese text where the concerned entity is being mis-written.\n",
    "\n",
    "For any good functioning NLU system, it would be ideal if the misspelled/mis-written words can be identified and corrected. In English, this is usually performed using some kind of edit distance operation (e.g. Levenshtein Distance) or phonetic correction (e.g. Soundex).\n",
    "\n",
    "While misspelled words in English doesn't affect the word boundary, a Chinese word with incorrect choice of characters can often leads to tokenisation problem, since word boundaries in Chinese text is partly determined using statistical models (e.g. HMM), which is based on the transition probability between different states (e.g. BMES) and words, usually derived by data mining on text with mostly correctly written words.\n",
    "\n",
    "As such miswritten Chinese word are much more troublesome to deal with when compared to misspelled words in English, since the standard misspelled correction operations used in English like Soundex and Levenshtein Distance wouldn't work on incorrectly tokenised words.\n",
    "\n",
    "The below example illustrate the problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['尖沙咀', '有', '咩', '好', '嘢', '食']\n",
      "['尖', '沙嘴', '有', '咩', '好', '嘢', '食']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "# from nltk.tokenize.stanford import CoreNLPTokenizer\n",
    "# sttok = CoreNLPTokenizer('http://localhost:9001')\n",
    "#sttok.tokenize(sent)\n",
    "sent = \"尖沙咀有咩好嘢食\"\n",
    "tok = jieba.cut(sent, False)\n",
    "print (list(tok))\n",
    "\n",
    "sent = \"尖沙嘴有咩好嘢食\"\n",
    "tok = jieba.cut(sent, False)\n",
    "print (list(tok))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had the word \"尖沙嘴\" been tokenised correctly, correction could have been easily performed. Unfortunately, this almost certainly won't happen especially with name entities (The fact that it is in Cantonese makes it even worse since the transition probability table used in Jieba is constructed using corpus of Standard Chinese writing.).\n",
    "\n",
    "Apart from using a machine-learning approach to deal with this problem (will be discussed in the end), in the following a rule-based matching approach is attempted and location entities were extracted by matching the corresponding jyutping romantisation of the entities. The below method does not required tokenisation and thus circumvented the issue mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is assumed that a list of the neccessary entities for the entity extraction task is obtainable.\n",
    "In this case, the location entities were extracted from a wikipedia page with the help of BeautifulSoup and some pre-processing, 360 locations in Hong Kong were extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "408\n",
      "['昂坪', '林村', '寻求帮助', '太平山 (香港)', 'Category:香港地方', '大浪灣 (香港島)', '对于来自此IP地址编辑的讨论[n]', '牛池灣村', '凹頭', '佐敦谷', '馬頭圍', '和合石', '髻山', '竹角', '葵芳', '赤柱', '牙鷹洲', '筆架山 (香港)', '小瀝源', '壁屋', '孟公屋', '梨木樹', '屏山 (香港)', '沙田圍', '上水', '大上托', '南灣 (香港島)', '觀塘', '牛頭角', '元朗市中心', '洪水橋', '山尞（页面不存在）', '鯉魚門', '長洲 (香港)', '新蒲崗', '蠔涌', '錦田', '打鼓嶺', '黃麻地', '打鼓嶺新村（页面不存在）', '中環', '港鐵東鐵線', '銅鑼灣 (沙田)', '西灣河', '提供当前新闻事件的背景资料', '官涌', '太和 (香港)', '深涌', '鎖羅盆', '油塘']\n",
      "381\n",
      "363\n",
      "['昂坪', '林村', '太平山', '大浪灣', '牛池灣村', '凹頭', '佐敦谷', '馬頭圍', '和合石', '髻山', '竹角', '葵芳', '赤柱', '牙鷹洲', '筆架山', '小瀝源', '壁屋', '孟公屋', '梨木樹', '屏山', '沙田圍', '上水', '大上托', '南灣', '觀塘', '牛頭角', '元朗市中心', '洪水橋', '山寮', '鯉魚門', '長洲', '新蒲崗', '蠔涌', '錦田', '打鼓嶺', '黃麻地', '打鼓嶺新村', '中環', '銅鑼灣', '西灣河', '官涌', '太和', '深涌', '鎖羅盆', '油塘', '大埔', '牛池灣', '石湖墟', '禮頓山', '灣仔']\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://zh.wikipedia.org/wiki/%E9%A6%99%E6%B8%AF%E5%9C%B0%E6%96%B9%E5%88%97%E8%A1%A8\")\n",
    "content = r.content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "e = soup.find_all(\"li\")\n",
    "#print (e)\n",
    "locs = []\n",
    "for li in e:\n",
    "    for a in li.find_all(\"a\"):\n",
    "        if a.get(\"title\"):\n",
    "            locs.append(a.get(\"title\"))\n",
    "                \n",
    "print (len(locs))\n",
    "locs = list(set(locs))\n",
    "print (len(locs))\n",
    "\n",
    "print (locs[:50])\n",
    "\n",
    "en_char = re.compile(\"[A-Za-z]\") # remove any location with alphabet e.g. 'Wikipedia:CC-BY-SA-3.0协议文本'\n",
    "\n",
    "locs = [loc for loc in locs if not en_char.findall(loc)] \n",
    "print (len(locs))\n",
    "\n",
    "brac1 = re.compile(\"(.*?) *\\(.*?\\)\") # remove the things in bracket e.g. 大浪灣 (香港島)\n",
    "brac2 = re.compile(\"(.*?) *（.*?）\") # remove the things in bracket e.g. 草灣（页面不存在）\n",
    "\n",
    "init_len = len(locs)\n",
    "locs = [loc if not brac1.findall(loc) else brac1.findall(loc)[0] for loc in locs]\n",
    "locs = [loc if not brac2.findall(loc) else brac2.findall(loc)[0] for loc in locs]\n",
    "assert init_len == len(locs)\n",
    "\n",
    "locs = [loc for loc in locs if len(loc)< 7] #remove unncessarily long string e.g. 关于本计划、你可以做什么、应该如何做'\n",
    "print (len(locs))\n",
    "locs = [loc if loc != \"山尞\" else \"山寮\" for loc in locs] # writing mistake in wikipedia\n",
    "locs = [loc for loc in locs if loc not in [\"寻求帮助\", \"沙咀\", \"港鐵東鐵線\"]]# it's not an actual place in HK (just a short-term for 沙咀懲教所)\n",
    "print (locs[:50])\n",
    "print (len(locs))\n",
    "#more pre-processing cleaning could be needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to find the corresponding jyutping romantisation of the location names. There are existing library such as Pycantonese to do that but it was found that there are many words that lacks jyutping information. Therefore in the end, a vocabulary of around 20,000 Chinese character with their corresponding jyutping romantisation was found online and subsequently used to create a jyutping dictionary. \n",
    "\n",
    "Note that many words has more than one jyutping representation and therefore the dictionary value is a list of jyutping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jyutping of around 20,000 chinese characters including cantonese characters\n",
    "import os  \n",
    "dir_ = 'tonghanma_yue'\n",
    "l = []\n",
    "file = open(dir_ + \"/\" + \"tonghanma_yue-yp.txt\", \"r\").read()\n",
    "f = file.split()\n",
    "l.extend(f)\n",
    "\n",
    "l = l[12:]\n",
    "\n",
    "word2jp ={}\n",
    "for idx in range(len(l)):\n",
    "    if len(l[idx]) == 1:\n",
    "        i = 1\n",
    "        jps = []\n",
    "        #print (idx)\n",
    "        try:\n",
    "            while len(l[idx+i]) !=1:\n",
    "                jps.append(l[idx+i])\n",
    "                i +=1\n",
    "        except IndexError:\n",
    "            pass\n",
    "        word2jp[l[idx]] = jps\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure there are no characters without jyutping\n",
    "for word in word2jp:\n",
    "    if word2jp[word] == []:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to compute the jyutping romantisation of the locations. Since some characters have more than 1 pronounciation, all possible sequences of pronounciation needs to be computed. This is done by using itertools. Note that the jyutping romantisation is consist of 4 parts, onset, nucleus, coda and tone. For this particular purpose, the tone is removed from the romantisation to account for the fact that mis-used character can have a different tone to the correct character. Also, removing the tone reduces the number of possible sequences and therefore improve computation speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "城門谷:  {(('s', 'i', 'ng'), ('m', 'u', 'n'), ('j', 'u', 'k')), (('s', 'e', 'ng'), ('m', 'u', 'n'), ('g', 'u', 'k')), (('s', 'i', 'ng'), ('m', 'u', 'n'), ('g', 'u', 'k')), (('s', 'e', 'ng'), ('m', 'u', 'n'), ('j', 'u', 'k'))}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pycantonese as pc\n",
    "def sent2jyutping(sent, remove_tone = False):\n",
    "    jps_seq = []\n",
    "    for char in sent:\n",
    "        try:\n",
    "            jps = word2jp[char] # a list of jyutping for that character\n",
    "            jps = [jp.replace(\"ngon\", \"on\") for jp in jps] # unified as they are very similar\n",
    "            jps_seq.append(list(set(jps))) \n",
    "\n",
    "        except KeyError:\n",
    "            print (char) #print if that character is not in the dictionary\n",
    "        possible_jps = list(itertools.product(*jps_seq)) \n",
    "        possible_jps = [\"\".join(jp) for jp in possible_jps] # construct all possible sequences of jyutping of that sentence\n",
    "        possible_jps = [tuple(pc.parse_jyutping(jp)) for jp in possible_jps] # a list of a tuple of tuple\n",
    "    if remove_tone == True:     \n",
    "        possible_jps = [tuple(tup[:3] for tup in jp) for jp in possible_jps]\n",
    "        possible_jps = set(possible_jps)         \n",
    "    return (possible_jps)\n",
    "\n",
    "\n",
    "loc2jp = {}\n",
    "for loc in locs:\n",
    "    loc2jp[loc] = sent2jyutping(loc, remove_tone = True)\n",
    "# some entity have more than one pronounciation\n",
    "print (\"城門谷: \", loc2jp[\"城門谷\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "sent = \"同鑼灣或者尖嘴有咩食\"\n",
    "q = sent2jyutping(sent,remove_tone = True)\n",
    "print (len(q)) # this sentence has 3 possible ways of pronouncing it (with tone removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entity extraction, the sentence is first corrected. This is done using the correction and match function below.\n",
    "\n",
    "Note that the occur_table is a fake occurence frequency table. In reality, this would be computed by counting the occurrence frequency of each entity in a large corpus. The occur_table help decide which entity is more likely when there are multiple entities that are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 點樣可以去士瓜灣\n",
      "corrected: 點樣可以去土瓜灣\n",
      "entities entity_tag:\n",
      "['土瓜灣'] oooooLLL\n",
      "original: 點樣可以去尖嘴\n",
      "corrected: 點樣可以尖沙咀\n",
      "entities entity_tag:\n",
      "['尖沙咀'] ooooLLL\n",
      "original: 點樣可以去同鑼灣\n",
      "corrected: 點樣可以去銅鑼灣\n",
      "entities entity_tag:\n",
      "['銅鑼灣'] oooooLLL\n",
      "original: 同鑼灣點樣去\n",
      "corrected: 銅鑼灣點樣去\n",
      "entities entity_tag:\n",
      "['銅鑼灣'] LLLooo\n",
      "original: 完朗有無得打邊爐\n",
      "corrected: 元朗有無得打邊爐\n",
      "entities entity_tag:\n",
      "['元朗'] LLoooooo\n",
      "original: 點樣可以由火碳去油唐\n",
      "corrected: 點樣可以由火炭去油塘\n",
      "entities entity_tag:\n",
      "['油塘', '火炭'] oooooLLoLL\n",
      "original: 天水偉邊度有壽司食\n",
      "corrected: 天水圍邊度有壽司食\n",
      "entities entity_tag:\n",
      "['天水圍'] LLLoooooo\n",
      "original: 同鑼灣或者全灣或者筲機灣或者牛投角或者中環有咩食\n",
      "corrected: 銅鑼灣或者荃灣或者筲箕灣或者牛頭角或者中環有咩食\n",
      "entities entity_tag:\n",
      "['牛頭角', '中環', '銅鑼灣', '筲箕灣', '荃灣'] LLLooLLooLLLooLLLooLLooo\n"
     ]
    }
   ],
   "source": [
    "# fake prob table\n",
    "occur_table = {\"尖沙咀\": 10,\n",
    "               \"尖鼻咀\": 1,\n",
    "               \"中環\": 10, \n",
    "               \"中灣\": 1,\n",
    "               \"爛角咀\":1,\n",
    "                \"大角咀\": 10,\n",
    "                \"黑角頭\": 1,\n",
    "                 \"牛頭角\": 10,\n",
    "                \"馬頭角\": 3,\n",
    "                \"沙頭角\": 5}\n",
    "\n",
    "def match(focused_jp = list, entity_dict = dict):\n",
    "    global jp2, loc2\n",
    "    max_precision = -1\n",
    "    most_probable_entity = None\n",
    "    for entity in entity_dict:\n",
    "        possible_jp_seq = entity_dict[entity] #a list of strings\n",
    "        for entity_jp in possible_jp_seq:\n",
    "            precision = sum([1 for tup in focused_jp if tup in entity_jp])/len(entity_jp)\n",
    "            if precision > 0.6:\n",
    "      #          entity_tags = entity_tags[:start_char] + len(focus_jp) * \"B\" + entity_tags[end_char+1:]\n",
    "                if precision > max_precision:\n",
    "                    max_precision = precision                                      \n",
    "                    most_probable_entity = entity\n",
    "                    break\n",
    "                elif precision == max_precision:\n",
    "                    #check occurance freq\n",
    "              #      print (precision, potential_loc, loc2)\n",
    "                    if occur_table[entity] > occur_table[most_probable_entity]:\n",
    "                        most_probable_entity = entity\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        pass # even if occur freq is the same\n",
    "    return most_probable_entity\n",
    "\n",
    "\n",
    "def correction(sent = str, entity_dict = dict):\n",
    "    \"\"\"\n",
    "    sent is a string.\n",
    "    entity_dict is a entity dictionary with values as tuple of tuples.\n",
    "    return the corrected sentence as string.\n",
    "    \"\"\"\n",
    "    ###First part is to find entity in the sentence by exact character match.\n",
    "    ###Second part is to find entity in the sentence with high precision phonetic match.\n",
    "    entity_tags = \"o\" * len(sent)\n",
    "    sent_jps = sent2jyutping(sent, remove_tone = True)\n",
    "    tag = \"T\" # an arbitary tag\n",
    "    #1 \n",
    "    # tag all entity with exact character match\n",
    "    for entity in entity_dict.keys():\n",
    "        if entity in sent:\n",
    "        #    print (entity)\n",
    "            start = sent.index(entity)\n",
    "            end = start + len(entity)\n",
    "            entity_tags = entity_tags[:start] + len(entity) * tag + entity_tags[end:]\n",
    "            sent = sent[:start] + entity + sent[end:]\n",
    "        #    print (sent)\n",
    "    #2\n",
    "    #  if precision > 0.6, match the word with the entity with highest precision\n",
    "    #  correct it then tag it\n",
    "    for entity in entity_dict:\n",
    "        possible_jp_seq = entity_dict[entity]\n",
    "        for entity_jp in possible_jp_seq: #loop through each jyutping sequence of the location\n",
    "            len_window = min(len(entity_jp), len(sent))  #number of tuple \n",
    "            for sent_jp in sent_jps: #loop through each jyutping sequence of the sentence\n",
    "                for idx in range(len(sent_jp)- len_window+1):\n",
    "                    focused_window = sent_jp[idx:idx+len_window] # list of tuples of part of the sequence\n",
    "                    if focused_window[0] == entity_jp[0] or focused_window[-1] == entity_jp[-1]:\n",
    "                        if all([True if t == \"o\" else False for t in entity_tags[idx:idx+len_window]]): \n",
    "                            #only proceed if there is no existing tag\n",
    "                            start_char = idx\n",
    "                            end_char = idx + len_window-1\n",
    "\n",
    "                            if focused_window[0] in entity_jp[0]:\n",
    "                                end_char = start_char + len_window -1\n",
    "                            elif focused_window[-1] in entity_jp[-1]:\n",
    "                                start_char = end_char - len_window +1\n",
    "\n",
    "                            precision = sum([1 for char in focused_window if char in entity_jp])/len_window                       \n",
    "                            if precision > 0.6:\n",
    "\n",
    "                                focused_window = sent_jp[start_char:end_char +1] ###\n",
    "                         #       print (focused_window)\n",
    "                         #       print (entity_jp)\n",
    "                                correct_entity = match(focused_window, entity_dict= entity_dict)\n",
    "                       #         print (correct_entity)\n",
    "\n",
    "                                if correct_entity != None:\n",
    "                                    entity_tags = entity_tags[:start_char] + len(correct_entity) * tag + entity_tags[end_char + 1:]\n",
    "                                    sent = sent[:start_char] + correct_entity + sent[end_char + 1:]\n",
    "                                    sent_jps = sent2jyutping(sent, remove_tone = True)\n",
    "                                else:\n",
    "                                    pass\n",
    "\n",
    "                        else:\n",
    "                            pass # have tag already                                      \n",
    "    return sent    \n",
    "def entities_extractor(sent, entity_dict = dict, tag = str, entity_tags = None):\n",
    "    \"\"\"\n",
    "    sent is a string.\n",
    "    entity_dict is a entity dictionary with values as tuple of tuples.\n",
    "    tag is a one character string.\n",
    "    entity_tags (Optional) is a string of length len(sent)\n",
    "    return the corrected sentence and the corresponding tag as string.\n",
    "    \"\"\"    \n",
    "    assert len(tag)== 1, \"tag must only be 1 character long in length\"\n",
    "    if entity_tags == None:\n",
    "        entity_tags = \"o\" * len(sent)\n",
    "    else:\n",
    "        assert type(entity_tags) == str and len(entity_tags) == len(sent), \"entity_tags must be a string of the same length as sent\"\n",
    "    entities = []\n",
    "    for entity in entity_dict.keys():\n",
    "        if entity in sent:\n",
    "            start = sent.index(entity)\n",
    "            end = start + len(entity)\n",
    "            if all(True for tag in entity_tags[start:end] if tag == \"o\"):\n",
    "                entity_tags = entity_tags[:start] + len(entity) * tag + entity_tags[end:]\n",
    "                entities.append(entity)\n",
    "    return entities, entity_tags\n",
    "\n",
    "sent1 = \"點樣可以去士瓜灣\"\n",
    "sent2 = \"點樣可以去尖嘴\"\n",
    "sent3 = \"點樣可以去同鑼灣\"\n",
    "sent4 = \"同鑼灣點樣去\"\n",
    "sent5 = \"完朗有無得打邊爐\"\n",
    "sent6 = \"點樣可以由火碳去油唐\"\n",
    "sent7 = \"天水偉邊度有壽司食\"\n",
    "\n",
    "sent8 = \"同鑼灣或者全灣或者筲機灣或者牛投角或者中環有咩食\" \n",
    "\n",
    "for sent in [sent1, sent2,sent3, sent4, sent5, sent6,sent7,sent8]:\n",
    "    print (\"original: \" + sent)\n",
    "    corrected_sent = correction(sent,loc2jp)\n",
    "    print (\"corrected: \" + corrected_sent)\n",
    "    entities, entity_tags = entities_extractor(corrected_sent, loc2jp, \"L\")\n",
    "    print (\"entities\", \"entity_tag:\")\n",
    "    print (entities, entity_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it can been seen that the above code can extract multiple entities from a text despite the entity is mis-written and each character corresponding to the entity in the text is tagged with \"L\" to indicate that it's a location entity.\n",
    "\n",
    "Note that a strong assumption is made that mis-written word are solely due to mis-used choice of characters as opposed to missing character. Specifically, if a word is missing a character, the correction function breaks down (see 點樣可以去尖嘴 above). \n",
    "\n",
    "The correction can get complicated if it needs to take into account the possibility of word with missing characters (althought it won't affect entity extraction result). In reality, most mis-written words are due to mis-used choice of character as opposed to missing character. Therefore, I decided to only deal with the prior issue for now. Taking this location entity extraction task as an example, 尖咀 strictly speaking isn't a miswritten word but more of a slang, so it might be more appropriate to add it into the list of entity.\n",
    "\n",
    "In summary, a phonetic matching-based approach to handle entity extraction in Cantonese text with mis-written words is shown. This method circumvent the troublesome problem manifested from incorrect tokenisation that exist in Chinese text when characters are wrongly used. Judging by observation, the method has a high precision rate as with any other rule-based NLU method. \n",
    "\n",
    "A potential problem with this method is scalability. For example, in reality people might want to ask for restaurant choices near a very specific location (e.g. providing a street name), a location that doesn't exist in our list of entity. \n",
    "\n",
    "An alternative approach would be to train a supervised machine learning model to tag entities. This however is not straightforward as written Cantonese text is not easy to find. With the available ~50,000 Cantonese Wikipedia articles and coupled with comments that can be found in Cantonese forum, it might be possible to extract adequate amount of Cantonese sentences containing locations. These could thus be used as labelled data to train a ML-model specifically for location entity tagging.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
