{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here multi-entity extractions was attempted using a phonetic matching approach on Cantonese text where the concerned entity is being mis-written.\n",
    "\n",
    "For any good functioning NLU system, it would be ideal if the misspelled/mis-written words can be identified and corrected. In English, this is usually performed using some kind of edit distance operation (e.g. Levenshtein Distance) or phonetic correction (e.g. Soundex).\n",
    "\n",
    "While misspelled words in English doesn't affect the word boundary, a Chinese word with incorrect choice of characters can often leads to tokenisation problem, since word boundaries in Chinese text is partly determined using statistical models (e.g. HMM), which is based on the transition probability between different states (e.g. BMES) and words, usually derived by data mining on text with mostly correctly written words.\n",
    "\n",
    "As such miswritten Chinese word are much more troublesome to deal with when compared to misspelled words in English, since the standard misspelled correction operations used in English like Soundex and Levenshtein Distance wouldn't work on incorrectly tokenised words.\n",
    "\n",
    "The below example illustrate the problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.583 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['尖沙咀', '有', '咩', '好', '嘢', '食']\n",
      "['尖', '沙嘴', '有', '咩', '好', '嘢', '食']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "# from nltk.tokenize.stanford import CoreNLPTokenizer\n",
    "# sttok = CoreNLPTokenizer('http://localhost:9001')\n",
    "#sttok.tokenize(sent)\n",
    "sent = \"尖沙咀有咩好嘢食\"\n",
    "tok = jieba.cut(sent, False)\n",
    "print (list(tok))\n",
    "\n",
    "sent = \"尖沙嘴有咩好嘢食\"\n",
    "tok = jieba.cut(sent, False)\n",
    "print (list(tok))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had the word \"尖沙嘴\" been tokenised correctly, correction could have been easily performed. Unfortunately, this almost certainly won't happen especially with name entities (The fact that it is in Cantonese makes it even worse since the transition probability table used in Jieba is constructed using corpus of Standard Chinese writing.).\n",
    "\n",
    "Apart from using a machine-learning approach to deal with this problem (will be discussed in the end), in the following a rule-based matching approach is attempted and location entities were extracted by matching the corresponding jyutping romantisation of the entities. The below method does not required tokenisation and thus circumvented the issue mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is assumed that a list of the neccessary entities for the entity extraction task is obtainable.\n",
    "In this case, the location entities were extracted from a wikipedia page with the help of BeautifulSoup and some pre-processing, 360 locations in Hong Kong were extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "408\n",
      "['城門谷', '洪聖爺灣', '白沙灣 (西貢)', '船灣', '港鐵東鐵線', '吉澳', '添馬艦 (香港)', '荃景圍', '沙灣 (香港)', '北角村', '十四鄉', '大坳門', '修頓', '樂安排（页面不存在）', '海下灣', '深水灣', '新蒲崗', '坪洲', '西營盤', '白沙頭', '蝴蝶灣', '渣甸山', '昂坪', '榕樹澳', '西灣河', '分流 (香港)', '十八鄉', '秀茂坪', '鑽石山', '坪石邨', '杏花邨', '赤柱', '金鐘', '大嶼山', '天水圍', '紅磡', '二澳', '深井', '林村', '東角', '愛秩序灣', '随机载入一个页面[x]', '参与维基百科社群的讨论', '和合石', '圓洲角', '查看连接的数据存储库项[g]', '梅窩', '打鼓嶺新村（页面不存在）', '关于此页面的讨论[t]', '鴨脷洲']\n",
      "381\n",
      "363\n",
      "['城門谷', '洪聖爺灣', '白沙灣', '船灣', '吉澳', '添馬艦', '荃景圍', '沙灣', '北角村', '十四鄉', '大坳門', '修頓', '樂安排', '海下灣', '深水灣', '新蒲崗', '坪洲', '西營盤', '白沙頭', '蝴蝶灣', '渣甸山', '昂坪', '榕樹澳', '西灣河', '分流', '十八鄉', '秀茂坪', '鑽石山', '坪石邨', '杏花邨', '赤柱', '金鐘', '大嶼山', '天水圍', '紅磡', '二澳', '深井', '林村', '東角', '愛秩序灣', '和合石', '圓洲角', '梅窩', '打鼓嶺新村', '鴨脷洲', '烏蛟騰', '青龍頭', '東平洲', '東頭邨', '土瓜灣']\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://zh.wikipedia.org/wiki/%E9%A6%99%E6%B8%AF%E5%9C%B0%E6%96%B9%E5%88%97%E8%A1%A8\")\n",
    "content = r.content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "e = soup.find_all(\"li\")\n",
    "#print (e)\n",
    "locs = []\n",
    "for li in e:\n",
    "    for a in li.find_all(\"a\"):\n",
    "        if a.get(\"title\"):\n",
    "            locs.append(a.get(\"title\"))\n",
    "                \n",
    "print (len(locs))\n",
    "locs = list(set(locs))\n",
    "print (len(locs))\n",
    "\n",
    "print (locs[:50])\n",
    "\n",
    "en_char = re.compile(\"[A-Za-z]\") # remove any location with alphabet e.g. 'Wikipedia:CC-BY-SA-3.0协议文本'\n",
    "\n",
    "locs = [loc for loc in locs if not en_char.findall(loc)] \n",
    "print (len(locs))\n",
    "\n",
    "brac1 = re.compile(\"(.*?) *\\(.*?\\)\") # remove the things in bracket e.g. 大浪灣 (香港島)\n",
    "brac2 = re.compile(\"(.*?) *（.*?）\") # remove the things in bracket e.g. 草灣（页面不存在）\n",
    "\n",
    "init_len = len(locs)\n",
    "locs = [loc if not brac1.findall(loc) else brac1.findall(loc)[0] for loc in locs]\n",
    "locs = [loc if not brac2.findall(loc) else brac2.findall(loc)[0] for loc in locs]\n",
    "assert init_len == len(locs)\n",
    "\n",
    "locs = [loc for loc in locs if len(loc)< 7] #remove unncessarily long string e.g. 关于本计划、你可以做什么、应该如何做'\n",
    "print (len(locs))\n",
    "locs = [loc if loc != \"山尞\" else \"山寮\" for loc in locs] # writing mistake in wikipedia\n",
    "locs = [loc for loc in locs if loc not in [\"寻求帮助\", \"沙咀\", \"港鐵東鐵線\"]]# it's not an actual place in HK (just a short-term for 沙咀懲教所)\n",
    "print (locs[:50])\n",
    "print (len(locs))\n",
    "#more pre-processing cleaning could be needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to find the corresponding jyutping romantisation of the location names. There are existing library such as Pycantonese to do that but it was found that there are many words that lacks jyutping information. Therefore in the end, a vocabulary of around 20,000 Chinese character with their corresponding jyutping romantisation was found online and subsequently used to create a jyutping dictionary. \n",
    "\n",
    "Note that many words has more than one jyutping representation and therefore the dictionary value is a list of jyutping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20061"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jyutping of around 20,000 chinese characters including cantonese characters\n",
    "import os  \n",
    "dir_ = 'tonghanma_yue'\n",
    "l = []\n",
    "file = open(dir_ + \"/\" + \"tonghanma_yue-yp.txt\", \"r\").read()\n",
    "f = file.split()\n",
    "l.extend(f)\n",
    "\n",
    "l = l[12:]\n",
    "\n",
    "word2jp ={}\n",
    "for idx in range(len(l)):\n",
    "    if len(l[idx]) == 1:\n",
    "        i = 1\n",
    "        jps = []\n",
    "        #print (idx)\n",
    "        try:\n",
    "            while len(l[idx+i]) !=1:\n",
    "                jps.append(l[idx+i])\n",
    "                i +=1\n",
    "        except IndexError:\n",
    "            pass\n",
    "        word2jp[l[idx]] = jps\n",
    "    else:\n",
    "        pass\n",
    "len(word2jp)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "㔌 ['sei3', 'tam3', 'zap3', 'zau6', 'zek3', 'zik3']\n",
      "䄘 ['lok6', 'pang4', 'ping1', 'teoi3', 'zaai3', 'zi4']\n",
      "䐑 ['bai3', 'dip6', 'pung3', 'sip3', 'sit3', 'zip3']\n",
      "䳬 ['aa3', 'bit3', 'fong2', 'gei3', 'tou4', 'zik6', 'zim2']\n",
      "䵎 ['caau3', 'doi6', 'sou1', 'teon1', 'toi5', 'zyun1']\n",
      "乜 ['mat1', 'me1', 'me2', 'me5', 'mi1', 'ne6']\n",
      "唅 ['ham1', 'ham3', 'ham4', 'ngam4', 'ngam6', 'ngan4']\n",
      "那 ['aa6', 'naa5', 'naa6', 'no1', 'no4', 'no5']\n"
     ]
    }
   ],
   "source": [
    "# making sure there are no characters without jyutping and print words with many pronounciation\n",
    "for word in word2jp:\n",
    "    if word2jp[word] == []:\n",
    "        print (word)\n",
    "    elif len(word2jp[word]) >5:\n",
    "        print (word, word2jp[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to compute the jyutping romantisation of the locations. Since some characters have more than 1 pronounciation, all possible sequences of pronounciation needs to be computed. This is done by using itertools. Note that the jyutping romantisation is consist of 4 parts, onset, nucleus, coda and tone. For this particular purpose, the tone is removed from the romantisation to account for the fact that mis-used character can have a different tone to the correct character. Also, removing the tone reduces the number of possible sequences and therefore improve computation speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "城門谷:  {(('s', 'e', 'ng'), ('m', 'u', 'n'), ('g', 'u', 'k')), (('s', 'e', 'ng'), ('m', 'u', 'n'), ('j', 'u', 'k')), (('s', 'i', 'ng'), ('m', 'u', 'n'), ('g', 'u', 'k')), (('s', 'i', 'ng'), ('m', 'u', 'n'), ('j', 'u', 'k'))}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pycantonese as pc\n",
    "def sent2jyutping(sent, remove_tone = False):\n",
    "    jps_seq = []\n",
    "    for char in sent:\n",
    "        try:\n",
    "            jps = word2jp[char] # a list of jyutping for that character\n",
    "            jps = [jp.replace(\"ngon\", \"on\") for jp in jps] # unified as they are very similar\n",
    "            jps_seq.append(list(set(jps))) \n",
    "\n",
    "        except KeyError:\n",
    "            print (char) #print if that character is not in the dictionary\n",
    "        possible_jps = list(itertools.product(*jps_seq)) \n",
    "        possible_jps = [\"\".join(jp) for jp in possible_jps] # construct all possible sequences of jyutping of that sentence\n",
    "        possible_jps = [tuple(pc.parse_jyutping(jp)) for jp in possible_jps] # a list of a tuple of tuple\n",
    "    if remove_tone == True:     \n",
    "        possible_jps = [tuple(tup[:3] for tup in jp) for jp in possible_jps]\n",
    "        possible_jps = set(possible_jps)         \n",
    "    return (possible_jps)\n",
    "\n",
    "\n",
    "loc2jp = {}\n",
    "for loc in locs:\n",
    "    loc2jp[loc] = sent2jyutping(loc, remove_tone = True)\n",
    "# some entity have more than one pronounciation\n",
    "print (\"城門谷: \", loc2jp[\"城門谷\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "sent = \"同鑼灣或者尖嘴有咩食\"\n",
    "q = sent2jyutping(sent,remove_tone = True)\n",
    "print (len(q)) # this sentence has 3 possible ways of pronouncing it (with tone removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entity extraction, the sentence is first corrected. This is done using the correction and match function below.\n",
    "\n",
    "Note that the occur_table is a fake occurence frequency table. In reality, this would be computed by counting the occurrence frequency of each entity in a large corpus. The occur_table help decide which entity is more likely when there are multiple entities that are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 點樣可以去士瓜灣\n",
      "corrected: 點樣可以去土瓜灣\n",
      "entities entity_tag:\n",
      "['土瓜灣'] oooooLLL\n",
      "original: 點樣可以去尖嘴\n",
      "corrected: 點樣可以尖沙咀\n",
      "entities entity_tag:\n",
      "['尖沙咀'] ooooLLL\n",
      "original: 點樣可以去同鑼灣\n",
      "corrected: 點樣可以去銅鑼灣\n",
      "entities entity_tag:\n",
      "['銅鑼灣'] oooooLLL\n",
      "original: 同鑼灣點樣去\n",
      "corrected: 銅鑼灣點樣去\n",
      "entities entity_tag:\n",
      "['銅鑼灣'] LLLooo\n",
      "original: 完朗有無得打邊爐\n",
      "corrected: 元朗有無得打邊爐\n",
      "entities entity_tag:\n",
      "['元朗'] LLoooooo\n",
      "original: 點樣可以由火碳去油唐\n",
      "corrected: 點樣可以由火炭去油塘\n",
      "entities entity_tag:\n",
      "['油塘', '火炭'] oooooLLoLL\n",
      "original: 天水偉邊度夫水偉有壽司食\n",
      "corrected: 天水圍邊度天水圍有壽司食\n",
      "entities entity_tag:\n",
      "['天水圍', '天水圍'] LLLooLLLoooo\n",
      "original: 同鑼灣或者全灣或者筲機灣或者牛投角或者中環或者全灣有咩食\n",
      "corrected: 銅鑼灣或者荃灣或者筲箕灣或者牛頭角或者中環或者荃灣有咩食\n",
      "entities entity_tag:\n",
      "['筲箕灣', '牛頭角', '荃灣', '荃灣', '銅鑼灣', '中環'] LLLooLLooLLLooLLLooLLooLLooo\n",
      "original: 九龍彎去由麻地快啲定北角去左敦快啲\n",
      "corrected: 九龍灣去油麻地快啲定北角去佐敦快啲\n",
      "entities entity_tag:\n",
      "['北角', '佐敦', '九龍灣', '油麻地'] LLLoLLLoooLLoLLoo\n"
     ]
    }
   ],
   "source": [
    "# fake occur_table for demonstration, in reality all entity occurence frequency is obtained from data mining.\n",
    "occur_table = {\"尖沙咀\": 10,\n",
    "               \"尖鼻咀\": 1,\n",
    "               \"中環\": 10, \n",
    "               \"中灣\": 1,\n",
    "               \"爛角咀\":1,\n",
    "                \"大角咀\": 10,\n",
    "                \"黑角頭\": 1,\n",
    "                 \"牛頭角\": 10,\n",
    "                \"馬頭角\": 3,\n",
    "                \"沙頭角\": 5}\n",
    "\n",
    "def match(focused_jp = list, entity_dict = dict):\n",
    "    max_precision = -1\n",
    "    most_probable_entity = None\n",
    "    for entity in entity_dict:\n",
    "        possible_jp_seq = entity_dict[entity] #a list of strings\n",
    "        precision = max([sum([1 for tup in focused_jp if tup in entity_jp])/len(entity_jp) for entity_jp in possible_jp_seq])\n",
    "        if precision > 0.6:\n",
    "            if precision > max_precision:\n",
    "                max_precision = precision                                      \n",
    "                most_probable_entity = entity\n",
    "         #       break\n",
    "            elif precision == max_precision:\n",
    "                #check occurance freq\n",
    "                try:\n",
    "                    if occur_table[entity] > occur_table[most_probable_entity]:\n",
    "                        most_probable_entity = entity\n",
    "                    else:\n",
    "                        pass # even if occur freq is the same\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    return most_probable_entity\n",
    "\n",
    "\n",
    "\n",
    "def correction(sent = str, entity_dict = dict):\n",
    "    \"\"\"\n",
    "    sent is a string.\n",
    "    entity_dict is a entity dictionary with values as tuple of tuples.\n",
    "    return the corrected sentence as string.\n",
    "    \"\"\"\n",
    "    ###First part is to find entity in the sentence by exact character match.\n",
    "    ###Second part is to find entity in the sentence with high precision phonetic match.\n",
    "    entity_tags = \"o\" * len(sent)\n",
    "    sent_jps = sent2jyutping(sent, remove_tone = True)\n",
    "    tag = \"T\" # an arbitary tag\n",
    "    #1 \n",
    "    # tag all entity with exact character match\n",
    "    for entity in entity_dict.keys():\n",
    "        if entity in sent:\n",
    "        #    print (entity)\n",
    "            start = sent.index(entity)\n",
    "            end = start + len(entity)\n",
    "            entity_tags = entity_tags[:start] + len(entity) * tag + entity_tags[end:]\n",
    "            sent = sent[:start] + entity + sent[end:]\n",
    "        #    print (sent)\n",
    "    #2\n",
    "    #  if precision > 0.6, match the word with the entity with highest precision\n",
    "    #  correct it then tag it\n",
    "    for entity in entity_dict:\n",
    "        possible_jp_seq = entity_dict[entity]\n",
    "        for entity_jp in possible_jp_seq: #loop through each jyutping sequence of the location\n",
    "            len_window = min(len(entity_jp), len(sent))  #number of tuple \n",
    "            for sent_jp in sent_jps: #loop through each jyutping sequence of the sentence\n",
    "                for idx in range(len(sent_jp)- len_window+1):\n",
    "                    focused_window = sent_jp[idx:idx+len_window] # list of tuples of part of the sequence\n",
    "                    if focused_window[0] == entity_jp[0] or focused_window[-1] == entity_jp[-1]:\n",
    "                        if all([True if t == \"o\" else False for t in entity_tags[idx:idx+len_window]]): \n",
    "                            #only proceed if there is no existing tag\n",
    "                            precision = sum([1 for char in focused_window if char in entity_jp])/len_window                       \n",
    "                            if precision > 0.6:\n",
    "                                start_char = idx\n",
    "                                end_char = start_char + len_window -1\n",
    "     #                           focused_window = sent_jp[start_char:end_char +1] ###\n",
    "                                correct_entity = match(focused_window, entity_dict= entity_dict)\n",
    "                                if correct_entity != None:\n",
    "                                    entity_tags = entity_tags[:idx] + len(correct_entity) * tag + entity_tags[end_char + 1:]\n",
    "                                    sent = sent[:start_char] + correct_entity + sent[end_char + 1:]\n",
    "                                    sent_jps = sent2jyutping(sent, remove_tone = True)\n",
    "                                else:\n",
    "                                    pass\n",
    "\n",
    "                        else:\n",
    "                            pass # have tag already                                      \n",
    "    return sent    \n",
    "def entities_extractor(sent, entity_dict = dict, tag = str, entity_tags = None):\n",
    "    \"\"\"\n",
    "    sent is a string.\n",
    "    entity_dict is a entity dictionary with values as tuple of tuples.\n",
    "    tag is a one character string.\n",
    "    entity_tags (Optional) is a string of length len(sent)\n",
    "    return the corrected sentence and the corresponding tag as string.\n",
    "    \"\"\"    \n",
    "    assert len(tag)== 1, \"tag must only be 1 character long in length\"\n",
    "    if entity_tags == None:\n",
    "        entity_tags = \"o\" * len(sent)\n",
    "    else:\n",
    "        assert type(entity_tags) == str and len(entity_tags) == len(sent), \"entity_tags must be a string of the same length as sent\"\n",
    "    entities = []\n",
    "    for entity in entity_dict.keys():\n",
    "        for match in re.finditer(entity, sent):\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            if all(True for tag in entity_tags[start:end] if tag == \"o\"):\n",
    "                entity_tags = entity_tags[:start] + len(entity) * tag + entity_tags[end:]\n",
    "                entities.append(entity)\n",
    "    return entities, entity_tags\n",
    "\n",
    "sent1 = \"點樣可以去士瓜灣\"\n",
    "sent2 = \"點樣可以去尖嘴\"\n",
    "sent3 = \"點樣可以去同鑼灣\"\n",
    "sent4 = \"同鑼灣點樣去\"\n",
    "sent5 = \"完朗有無得打邊爐\"\n",
    "sent6 = \"點樣可以由火碳去油唐\"\n",
    "sent7 = \"天水偉邊度夫水偉有壽司食\"  #same entity appear more than once\n",
    "sent8 = \"同鑼灣或者全灣或者筲機灣或者牛投角或者中環或者全灣有咩食\"\n",
    "sent9 = \"九龍彎去由麻地快啲定北角去左敦快啲\"\n",
    "\n",
    "for sent in [sent1, sent2,sent3, sent4, sent5, sent6,sent7,sent8, sent9]:\n",
    "    print (\"original: \" + sent)\n",
    "    corrected_sent = correction(sent,loc2jp)\n",
    "    print (\"corrected: \" + corrected_sent)\n",
    "    entities, entity_tags = entities_extractor(corrected_sent, loc2jp, \"L\")\n",
    "    print (\"entities\", \"entity_tag:\")\n",
    "    print (entities, entity_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can been seen that the above code can extract multiple entities from a text despite the entity is mis-written and each character corresponding to the entity in the text is tagged with \"L\" to indicate that it's a location entity.\n",
    "\n",
    "This method was able to make the correct correction even for misused character that sound phonetically different to the correct character (e.g. 夫水偉, 士瓜灣 above). Note that this is achieved by selecting the entity with highest precision while precision is above 0.6. When there are more than one entity with same precision, the one with the highest occurency frequency in the occur_table is selected.\n",
    "\n",
    "Note that a strong assumption is made that miswritten word are solely due to misused choice of characters as opposed to missing character. Specifically, if a word is missing a character, the correction function breaks down (see 點樣可以去尖嘴 above, note that entity extraction result is not affected). \n",
    "\n",
    "The correction can get complicated if it needs to take into account the possibility of word with missing characters. In reality, most mis-written words are due to mis-used choice of character as opposed to missing character. Therefore, I decided to only deal with the prior issue for now. Taking this location entity extraction task as an example, 尖咀 strictly speaking isn't a miswritten word but more of a slang, so it might be more appropriate to add it into the list of entity.\n",
    "\n",
    "In summary, a phonetic matching-based approach to handle entity extraction in Cantonese text with miswritten words is shown. This method circumvent the troublesome problem manifested from incorrect tokenisation that exist in Chinese text when characters are wrongly used. Judging by observation, the method has a high precision rate as with any other rule-based entity extraction method. \n",
    "\n",
    "A potential problem with this method is scalability. For example, in reality people might want to ask for restaurant choices near a very specific location (e.g. providing a street name or a smaller district), a location that doesn't exist in our list of entity. \n",
    "\n",
    "An alternative approach would be to train a supervised machine learning model to tag entities. With the available ~50,000 Cantonese Wikipedia articles and coupled with comments that can be found in Cantonese forum, it might be possible to extract adequate amount of Cantonese sentences containing locations. These could thus be treated as a multi-labelled classification problem to train a ML-model specifically for location entity tagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
